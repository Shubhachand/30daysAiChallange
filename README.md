Welcome to my journey through the **30 Days of AI Voice Agents** challenge! üöÄ  
This repo tracks my progress from **Day 1 to Day 30**, building AI-powered voice apps using **FastAPI**, **AssemblyAI**, and other tools. Each day, we build something new ‚Äì from simple TTS to real-time transcription bots.

---

## üìÖ Progress Overview

| Day | Task Description                             | Status  |
|-----|----------------------------------------------|---------|
| ‚úÖ 1 | Setup FastAPI server & UI                    | ‚úÖ Done |
| ‚úÖ 2 | Integrated Text-to-Speech (TTS) using Murf   | ‚úÖ Done |
| ‚úÖ 3 | UI for Echo Bot with recording               | ‚úÖ Done |
| ‚úÖ 4 | Visualize live audio waveform                | ‚úÖ Done |
| ‚úÖ 5 | Upload audio to server via endpoint          | ‚úÖ Done |
| ‚úÖ 6 | Transcribe audio using AssemblyAI            | ‚úÖ Done |
| ‚úÖ 7 | Echo Bot v2 ‚Äì Record ‚Üí Transcribe ‚Üí AI Voice | ‚úÖ Done |
| ‚úÖ 9 | Full non-streaming AI conversation pipeline  | ‚úÖ Done |
| ‚úÖ 10| Automatic continuous conversations           | ‚úÖ Done |
| ‚úÖ 11| Error handling & graceful recovery           | ‚úÖ Done |
| ‚úÖ 12| UI revamp & conversation flow upgrade        | ‚úÖ Done |
| ‚úÖ 13 | Create README.md documentation              | ‚úÖ Done |
| ‚úÖ 14 | Refactor code and upload to GitHub          | ‚úÖ Done |
| ‚úÖ 15 | Implement a basic WebSocket echo server     | ‚úÖ Done |
| ‚úÖ 16 | Stream audio from client to server          | ‚úÖ Done |
| ‚úÖ 17 | Transcribe streaming audio with AssemblyAI  | ‚úÖ Done |
| ‚úÖ 18 | Implement AssemblyAI turn detection         | ‚úÖ Done |
| ‚úÖ 19 | Get streaming LLM responses                 | ‚úÖ Done |
| ‚úÖ 20 | Stream text to Murf for TTS via WebSockets  | ‚úÖ Done |
| ‚úÖ 21 | Stream TTS audio from server to client      | ‚úÖ Done |
| ‚úÖ 22 | Play streaming audio on the client          | ‚úÖ Done |
| ‚úÖ 23 | Integrate all parts into a voice agent      | ‚úÖ Done |
| ‚úÖ 24 | Add a persona to the agent                  | ‚úÖ Done |
| ‚úÖ 25 | Add a special skill (e.g., web search)      | ‚úÖ Done |
| ‚úÖ 26 | Add a second special skill                  | ‚úÖ Done |
| ‚úÖ 27 | UI revamp and allow user API keys           | ‚úÖ Done |
| ‚úÖ 28 | Deploy the agent                            | ‚úÖ Done |
| ‚úÖ 29 | Update final documentation/README.md        | ‚úÖ Done |

## üî• What We Built So Far

### ‚úÖ Day 1 ‚Äì Project Setup & UI Bootstrapping
- Initialized FastAPI backend
- Created clean responsive UI using HTML & CSS
- Setup file structure: `static/`, `templates/`, `main.py`

---

### ‚úÖ Day 2 ‚Äì Text-to-Speech with Murf.ai
- Connected Murf API to backend
- Built `/generate` endpoint
- User enters text ‚Üí Audio is generated
- Audio is playable from the UI

---

### ‚úÖ Day 3 ‚Äì Echo Bot Recording UI
- Added recording functionality using `MediaRecorder` API
- Start/Stop buttons to control audio recording
- Saved audio locally in memory (no server upload yet)

---

### ‚úÖ Day 4 ‚Äì Audio Waveform Visualizer
- Used `Canvas` to show live audio waveform
- Integrated `AnalyserNode` from Web Audio API
- Made UI futuristic with pulse animations üéß

---

### ‚úÖ Day 5 ‚Äì Upload Audio to Server
- Built `/upload-audio` POST endpoint
- Saved uploaded audio file in `/uploads` directory
- Returned file name, type, and size from server
- Displayed upload status in frontend

---

### ‚úÖ Day 6 ‚Äì Transcribe Audio via AssemblyAI
- Used AssemblyAI Python SDK
- Created `/transcribe/file` endpoint that:
  - Accepts audio file
  - Transcribes using `transcribe(audio_data)`
  - Returns full transcript
- Displayed transcription on frontend after recording

---
### ‚úÖ  Day 7 ‚Äì Echo Bot v2 üé§‚ú®

- Record audio in the browser
- Upload to backend via /tts/echo endpoint
 - Transcribe audio using AssemblyAI
 - Generate AI voice from transcription using Murf API
 - Play back the AI-generated voice in the browser
 - It‚Äôs like talking to yourself‚Ä¶ but in a perfect studio voice üéôÔ∏è
---
### ‚úÖ Day 8 ‚Äì LLM Integration with Google Gemini ü§ñ
- Added `/llm/query` endpoint in FastAPI backend  
- Connected to Google‚Äôs Gemini API for intelligent, context-aware replies  
- Successfully tested ‚Äî the LLM can explain Murf AI features üìù‚ú®  
- Prepares the bot for natural, conversational responses  

---

### ‚úÖ Day 9 ‚Äì Full Non-Streaming AI Conversation Pipeline üéØ
- Implemented **listen ‚Üí think ‚Üí talk back** loop:  
  1. üé§ Record voice in browser  
  2. ‚úçÔ∏è Transcribe via AssemblyAI  
  3. üí° Generate smart response via Gemini API  
  4. üé∂ Convert to natural speech using Murf AI  
  5. üîÅ Play AI‚Äôs voice back instantly  
- Now holds voice-only conversations without typing  

---

### ‚úÖ Day 10 ‚Äì Automatic Continuous Conversations üîÑ
- Bot now **listens, thinks, and responds** without extra clicks  
- After speaking, it starts listening again  
- Smooth, delay-free, back-and-forth interaction

---

### ‚úÖ Day 11 ‚Äì Error Handling & Graceful Recovery üí™
- Handled API timeouts, connection drops, and unexpected failures  
- Bot **recovers gracefully** without freezing  
- Friendly fallback messages when something goes wrong

---

### ‚úÖ Day 13 ‚Äì Documentation with README.md üìÑ
- Created a comprehensive `README.md` in the project root
- Documented project purpose, features, architecture, and tech stack
- Added step-by-step setup, dependency installation, and API key instructions

---

### ‚úÖ Day 14 ‚Äì Code Refactoring & GitHub Upload üõ†Ô∏è
- Refactored codebase for clarity and maintainability
- Moved third-party service logic (STT, TTS) to `/services` folder
- Introduced Pydantic models for API schemas
- Removed unused code and cleaned up imports
- Uploaded the project to a public GitHub repository

---

### ‚úÖ Day 15 ‚Äì Basic WebSocket Implementation üåê
- Added `/ws` endpoint to FastAPI for WebSocket connections
- Implemented simple echo functionality (server returns received messages)
- Tested WebSocket with Postman and browser client

---

### ‚úÖ Day 16 ‚Äì Streaming Audio from Client to Server üéôÔ∏è
- Updated client to stream audio chunks to server via WebSocket in real-time
- Modified server to receive/process binary audio data
- Saved streamed audio as a file on the server

---

### ‚úÖ Day 17 ‚Äì Real-time Transcription with AssemblyAI üìù
- Integrated AssemblyAI SDK into WebSocket handler
- Streamed incoming audio directly to AssemblyAI‚Äôs real-time transcription
- Printed live transcription results to server console

---

### ‚úÖ Day 18 ‚Äì Implementing Turn Detection üîÑ
- Used AssemblyAI‚Äôs turn detection to identify when user stops speaking
- Sent final transcript back to client at end of each turn
- Displayed transcript in UI for seamless conversation flow

---

### ‚úÖ Day 19 ‚Äì Streaming Responses from LLM ü§ñ
- Sent user transcript to Google Gemini LLM for response
- Configured LLM API for streaming text responses
- Accumulated and logged response chunks on server

---

### ‚úÖ Day 20 ‚Äì Streaming Text-to-Speech with Murf üîä
- Established WebSocket connection to Murf TTS API
- Streamed LLM response text to Murf in chunks
- Received and logged base64-encoded audio stream from Murf

---

### ‚úÖ Day 21 ‚Äì Streaming Audio Back to the Client üîÅ
- Relayed audio chunks from Murf to client via WebSocket
- Client accumulated incoming audio data for playback
- Verified data transfer with browser console logs

---

### ‚úÖ Day 22 ‚Äì Real-time Audio Playback üéß
- Implemented client-side streaming audio playback
- Used Web Audio API to queue and play audio chunks as they arrived
- Achieved smooth, continuous conversational experience

---

### ‚úÖ Day 23 ‚Äì Full End-to-End Integration üöÄ
- Connected all components for a complete conversational voice agent
- Flow: real-time transcription ‚Üí LLM response ‚Üí TTS ‚Üí streaming playback
- Added chat history saving for session review

---

### ‚úÖ Day 24 ‚Äì Adding an Agent Persona ü¶∏
- Customized system prompt for LLM to define agent persona (e.g., pirate, robot)
- Ensured responses matched chosen persona‚Äôs tone and style

---

### ‚úÖ Day 25 ‚Äì Adding a Special Skill (Function Calling) üõ†Ô∏è
- Integrated LLM function-calling for special skills (e.g., web search via Tavily API)
- Enabled agent to answer questions using external tools

---

### ‚úÖ Day 26 ‚Äì Adding a Second Special Skill üß©
- Expanded agent with an additional skill (e.g., weather lookup)
- Broadened agent‚Äôs ability to handle complex, multi-domain queries

---

### ‚úÖ Day 27 ‚Äì UI Revamp and Configuration ‚öôÔ∏è
- Improved UI with a configuration section for user API keys
- Allowed users to enter their own Murf, AssemblyAI, and Gemini keys
- Enhanced flexibility and user control

---

### ‚úÖ Day 28 ‚Äì Deploying the Agent üåç
- Finalized dependencies and production configs
- Deployed FastAPI backend and frontend to Render
- Shared public URL for live demo

---

### ‚úÖ Day 29 ‚Äì Final Documentation Update üìù
- Thoroughly updated `README.md` with all new features and instructions
- Documented persona, special skills, and user-configurable API keys
- Polished setup and deployment steps for end users

---


## üõ† Tech Stack

- **Frontend**: HTML5, CSS3, JavaScript (vanilla)
- **Backend**: Python, FastAPI
- **APIs**: Murf.ai (TTS), AssemblyAI (Transcription)
- **Audio**: MediaRecorder API, Web Audio API, Canvas
- **Dev Tools**: VS Code, Postman, Git

---

## ‚öôÔ∏è How to Run Locally

### üöÄ Step-by-Step Guide for Beginners

#### 1Ô∏è‚É£ Clone the Repository
```bash
git clone https://github.com/your-username/ai-voice-agent.git
cd ai-voice-agent
```

#### 2Ô∏è‚É£ Create Virtual Environment
**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Mac/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

> üí° **Tip:** You'll see `(venv)` in your terminal when the virtual environment is active!

#### 3Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt
```

#### 4Ô∏è‚É£ Set Up Environment Variables
Create a `.env` file in the root directory:
```bash
# Windows
echo. > .env

# Mac/Linux
touch .env
```

Add your API keys to the `.env` file:
```env
MURF_API_KEY=your_murf_api_key_here
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
```

> üîë **Where to get API keys:**
> - **Murf.ai**: Sign up at [murf.ai](https://murf.ai)
> - **AssemblyAI**: Create account at [assemblyai.com](https://www.assemblyai.com)
> - **Google Gemini**: Get key from [Google AI Studio](https://makersuite.google.com/app/apikey)
> - **Tavily (Web Search Skill)**: Register for a free API key at [tavily.com](https://www.tavily.com)

#### 5Ô∏è‚É£ Start the FastAPI Server
```bash
```bash
uvicorn app.main:app --reload
```
```

> ‚úÖ **Success!** You should see: `Uvicorn running on http://127.0.0.1:8000`

#### 6Ô∏è‚É£ Open in Browser
Navigate to: [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

### üõ†Ô∏è Troubleshooting Tips

**If you get "pip not found":**
- Windows: `python -m pip install --upgrade pip`
- Mac/Linux: `python3 -m pip install --upgrade pip`

**If port 8000 is busy:**
```bash
uvicorn app.main:app --reload --port 8001

```

**To deactivate virtual environment:**
```bash
deactivate
```

### üì± Quick Start Commands (Copy & Paste)
```bash
# All-in-one commands for Windows
git clone https://github.com/Shubhachand/30daysAiChallange.git && cd 30daysAiChallange && python -m venv venv && venv\Scripts\activate && pip install -r requirements.txt

# All-in-one commands for Mac/Linux
git clone https://github.com/Shubhachand/30daysAiChallange.git && cd 30daysAiChallange && python3 -m venv venv && source venv/bin/activate && pip install -r requirements.txt
```




---

## üöÄ Final Goal (Day 30)

- üéØ Build a **complete AI Voice Agent app**
- üß† Features: TTS + Echo + Transcription + Real-time streaming
- üåê Deploy app on Render / Vercel
- üì∏ Post final demo on [LinkedIn](https://www.linkedin.com/in/shubhachand/)

---


## üì¢ Stay Tuned

> More updates daily...  
Follow along as I turn this into a fully functional **Voice Assistant App**!  
Made with ‚ù§Ô∏è and curiosity.

---

### üë®‚Äçüíª Author

**Shubhachand Patel**  
üéì Final year student | Full-stack & AI enthusiast  
üîó [LinkedIn](https://www.linkedin.com/in/shubhachand/) 

---

)

---

## üì¢ Stay Tuned

> More updates daily...  
Follow along as I turn this into a fully functional **Voice Assistant App**!  
Made with ‚ù§Ô∏è and curiosity.

---

